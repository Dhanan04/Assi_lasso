{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b82ccc07-e983-4999-8f58-1a77a74bae56",
   "metadata": {},
   "source": [
    "Q1. ans\n",
    "\n",
    "Lasso Regression, also known as L1 regularization, is a linear regression technique used for feature selection and regularization.it adds a penalty term to the cost function based on the absolute values of the coefficients. The main objective of Lasso Regression is to perform both regression and variable selection by driving some coefficients to exactly zero. \n",
    "\n",
    "Cost function for Lasso Regression:\n",
    "Cost = Sum of squared residuals + λ * (sum of absolute values of coefficients) \n",
    "λ (lambda) is the regularization parameter, it controls the strength of the penalty. A higher value of λ leads to more regularization\n",
    "\n",
    "Lasso and Ridge Regression (L2 regularization) both add penalty terms to the cost function, but they differ in the type of penalty. Lasso uses the absolute values of the coefficients (L1 penalty), while Ridge uses the squared values of the coefficients (L2 penalty). As a result, Lasso tends to produce sparser models with some coefficients exactly equal to zero, effectively performing feature selection, whereas Ridge generally produces smaller but non-zero coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b65a466-93e3-4be9-9bfe-45abe1eefc36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0819f3b-b358-4d47-8e16-0bc947455d37",
   "metadata": {},
   "source": [
    "Q2. ans\n",
    "\n",
    "The main advantage of using Lasso Regression in feature selection is its ability to perform automatic and efficient selection of relevant features while effectively reducing the impact of irrelevant or redundant ones. This property makes Lasso Regression a powerful tool for data scientists and analysts when dealing with high-dimensional datasets. it help in feature selection as well as countering overfittng "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1837e688-4824-4c4f-b0e4-0a04de1b039d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b203006-7d76-49d0-a16a-33de546e2ac4",
   "metadata": {},
   "source": [
    "Q3 .ans\n",
    "\n",
    "In Lasso Regression, some coefficients are exactly zero, which means the corresponding features are effectively excluded from the model. The non-zero coefficients represent the selected features and their importance in predicting the target variable the value which is zero is dropped as a feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e850a182-336a-43e6-8784-6d661f520edd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c976cd2a-a158-4a56-be34-f0c9f6b0ce1b",
   "metadata": {},
   "source": [
    "Q4. ans\n",
    "\n",
    "In Lasso Regression, there is one primary tuning parameter that can be adjusted, which is the regularization parameter (λ or alpha). The regularization parameter controls the strength of the penalty applied to the coefficients during the model training process. It determines the trade-off value between fitting the training data well (reducing the sum of squared residuals) and keeping the model simple by driving some coefficients to exactly zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd9d4e9-99aa-4bc4-b167-92e6fb1dfa6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80b858b9-54fe-4b7b-b403-c6efb1b8ecc5",
   "metadata": {},
   "source": [
    "Q5. ans\n",
    "\n",
    "Yes, Lasso Regression can be extended to handle non-linear regression problems through \"feature engineering\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15e4a17-33fd-41bf-891b-fe172e57d3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "086ede42-590b-41cb-a9e6-b5beb0661e2e",
   "metadata": {},
   "source": [
    "Q6. ans\n",
    "\n",
    "Ridge Regression adds a penalty term to the cost function based on the squared magnitudes of the coefficients. This penalty is proportional to the sum of squared coefficients, which is why it is also known as L2 regularization.\n",
    "\n",
    "Lasso Regression, on the other hand, adds a penalty term to the cost function based on the absolute magnitudes of the coefficients. This penalty is proportional to the sum of absolute values of the coefficients, which is why it is also known as L1 regularization.\n",
    "\n",
    "Ridge Regression and Lasso Regression are both linear regression techniques that introduce regularization to improve model performance and handle potential issues like multicollinearity and overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5e4c87-56a8-4e5f-a89c-e2224263028e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54f7b39b-3c98-42b6-bf4f-4310515d9e1d",
   "metadata": {},
   "source": [
    "Q7. ans \n",
    "\n",
    "Multicollinearity occurs when two or more predictor variables in a regression model are highly correlated, which can lead to instability and unreliable estimates of the coefficients in traditional linear regression models. lesso regression When faced with highly correlated features, it tends to select one feature and drives the coefficients of the remaining correlated features to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9809959-792d-49ef-a29e-4a037dfc1e09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00217faf-7154-42db-a659-43f7121c7044",
   "metadata": {},
   "source": [
    "Q8. ans \n",
    "\n",
    "The process of finding the optimal value of λ involves evaluating the model's performance on different subsets of the data for different values of λ. Cross-validation is a commonly used technique to accomplish this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8a97b0-49f8-48ca-b8de-ad3396607ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
